# Multi-layer Perceptron Implementation

A comprehensive implementation of Multi-layer Perceptrons (MLPs) from scratch and using PyTorch.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Results](#results)
- [Implementation Details](#implementation-details)

## ğŸ¯ Overview

This project implements two different approaches to Multi-layer Perceptrons:

1. **From Scratch Implementation (Task 1)**: Complete MLP implementation using only NumPy
   - Custom activation functions (ReLU, Sigmoid, Tanh, Leaky ReLU, ELU)
   - Forward and backward propagation
   - Binary cross-entropy loss
   - Gradient descent optimization

2. **PyTorch Implementation (Task 2)**: Modern deep learning approach
   - Custom dataset class for MNIST
   - Flexible MLP architecture with dropout
   - Training and validation pipelines
   - Hyperparameter optimization
   - Comprehensive evaluation metrics

## ğŸ“ Project Structure

```
mlp_implementation/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”œâ”€â”€ activations.py           # Activation functions (from scratch)
â”‚   â”œâ”€â”€ mlp_from_scratch.py     # Complete MLP implementation (NumPy)
â”‚   â””â”€â”€ pytorch_mlp.py          # PyTorch MLP implementation
â”œâ”€â”€ data/                        # Dataset directory (download required)
â”œâ”€â”€ results/                     # Training results and plots
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                   # This file
```

## âœ¨ Features

### From Scratch Implementation
- âœ… 5 activation functions with derivatives
- âœ… Two-layer MLP architecture
- âœ… Binary cross-entropy loss
- âœ… Gradient descent optimization
- âœ… XOR problem validation
- âœ… PyTorch compatibility verification

### PyTorch Implementation
- âœ… Custom MNIST dataset loader
- âœ… Configurable MLP architecture
- âœ… Dropout regularization
- âœ… Training and validation loops
- âœ… Hyperparameter grid search
- âœ… Confusion matrix computation
- âœ… Loss curve visualization
- âœ… Comprehensive evaluation metrics

## ğŸš€ Installation

1. **Clone the repository:**
   ```bash
   git clone <your-repo-url>
   cd mlp_implementation
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Download MNIST dataset:**
   ```bash
   # The dataset will be automatically downloaded when running the PyTorch implementation
   # Or manually download
   ```

## ğŸ’» Usage

### From Scratch Implementation

```python
from src import forward_propagation, compute_loss, backward_propagation, update_parameters
import numpy as np

# Example usage for XOR problem
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# Initialize parameters
W1 = np.random.randn(2, 3) * 0.1
b1 = np.zeros((1, 3))
W2 = np.random.randn(3, 1) * 0.1
b2 = np.zeros((1, 1))

# Training loop
for epoch in range(1000):
    p_i, cache = forward_propagation(X, W1, b1, W2, b2)
    loss = compute_loss(y, p_i)
    gradients = backward_propagation(X, y, cache, W2)
    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, gradients, 0.01)
    
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")
```

### PyTorch Implementation

```python
from src import MNISTCustomDataset, CustomMLP, train, validate, hyperparameter_search
import torch
from torch.utils.data import DataLoader
from torchvision import transforms

# Define transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

# Create datasets
train_dataset = MNISTCustomDataset('data/training', transform=transform)
val_dataset = MNISTCustomDataset('data/validation', transform=transform)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Create model
model = CustomMLP(input_size=784, output_size=10, dropout_rate=0.5)

# Hyperparameter search
learning_rates = [0.0001, 0.001, 0.01]
dropout_rates = [0.1, 0.2, 0.5]
results = hyperparameter_search(learning_rates, dropout_rates, train_loader, val_loader, 784, 10, 10, 'cpu')
```

## ğŸ“Š Results

### From Scratch Implementation
- **XOR Problem**: Successfully solved with 100% accuracy
- **Activation Functions**: All 5 functions implemented and validated
- **Gradient Verification**: Perfect match with PyTorch gradients

### PyTorch Implementation
- **Best Hyperparameters**: Learning Rate = 0.0001, Dropout Rate = 0.1
- **Training Accuracy**: 98.95%
- **Validation Accuracy**: 97.07%
- **Test Accuracy**: 96.89%

### Hyperparameter Analysis
The hyperparameter search revealed important insights:

- **High dropout rates** (0.8) lead to underfitting due to excessive regularization
- **Low dropout rates** (0.1) provide optimal regularization without overfitting
- **Low learning rates** (0.0001) enable stable convergence
- **High learning rates** (0.1) cause unstable training and poor performance

## ğŸ”§ Implementation Details

### Activation Functions
All activation functions are implemented with both forward and backward passes:

- **ReLU**: `max(0, x)`
- **Sigmoid**: `1/(1 + e^(-x))`
- **Tanh**: `(e^x - e^(-x))/(e^x + e^(-x))`
- **Leaky ReLU**: `x if x >= 0 else Î±x`
- **ELU**: `x if x >= 0 else Î±(e^x - 1)`

### MLP Architecture (PyTorch)
```
Input (784) â†’ Linear(512) â†’ ReLU â†’ Dropout(0.1) â†’ Linear(256) â†’ ReLU â†’ Linear(10)
```

### Training Process
1. **Data Loading**: Custom dataset with proper transformations
2. **Model Initialization**: Configurable architecture with dropout
3. **Training Loop**: Forward pass, loss computation, backpropagation
4. **Validation**: Performance monitoring on validation set
5. **Hyperparameter Tuning**: Grid search over learning rates and dropout rates
6. **Evaluation**: Confusion matrix and accuracy reporting

## ğŸ§ª Testing

The implementation includes comprehensive validation:

- **Gradient Verification**: All gradients match PyTorch implementations
- **XOR Problem**: Binary classification validation
- **MNIST Classification**: Multi-class classification on real dataset
- **Hyperparameter Search**: Systematic evaluation of model configurations

## ğŸ“ˆ Performance Analysis

The results demonstrate the importance of proper hyperparameter tuning:

1. **Learning Rate Impact**: 
   - Too high (0.1): Unstable training, poor convergence
   - Optimal (0.0001): Stable convergence, best performance

2. **Dropout Impact**:
   - Too high (0.8): Underfitting, reduced learning capacity
   - Optimal (0.1): Good regularization without overfitting

3. **Model Performance**:
   - Achieved 96.89% test accuracy on MNIST
   - Confusion matrix shows good class separation
   - Training curves show healthy convergence




*This implementation demonstrates both theoretical understanding of neural networks and practical deep learning skills using modern frameworks.*
