{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLP from Scratch Implementation\n",
        "\n",
        "This notebook demonstrates the implementation of a Multi-layer Perceptron from scratch using only NumPy, including the XOR problem as a validation test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "from mlp_from_scratch import forward_propagation, compute_loss, backward_propagation, update_parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XOR Dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "print(\"XOR Dataset:\")\n",
        "print(\"Input (X):\")\n",
        "print(X)\n",
        "print(\"\\nTarget (y):\")\n",
        "print(y.flatten())\n",
        "print(\"\\nXOR Truth Table:\")\n",
        "print(\"Input 1 | Input 2 | Output\")\n",
        "print(\"--------|---------|-------\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"   {X[i,0]}    |    {X[i,1]}    |   {y[i,0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model parameters\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1000\n",
        "\n",
        "# Initialize parameters with small random values\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "print(\"Model Architecture:\")\n",
        "print(f\"Input layer: {input_size} neurons\")\n",
        "print(f\"Hidden layer: {hidden_size} neurons\")\n",
        "print(f\"Output layer: {output_size} neuron\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Number of epochs: {num_epochs}\")\n",
        "\n",
        "print(f\"\\nInitial weights W1 shape: {W1.shape}\")\n",
        "print(f\"Initial bias b1 shape: {b1.shape}\")\n",
        "print(f\"Initial weights W2 shape: {W2.shape}\")\n",
        "print(f\"Initial bias b2 shape: {b2.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "print(\"Training MLP on XOR problem...\")\n",
        "print(\"Epoch\\tLoss\\t\\tPredictions\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "losses = []\n",
        "predictions_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward propagation\n",
        "    p_i, cache = forward_propagation(X, W1, b1, W2, b2)\n",
        "    loss = compute_loss(y, p_i)\n",
        "    losses.append(loss)\n",
        "    \n",
        "    # Backward propagation\n",
        "    gradients = backward_propagation(X, y, cache, W2)\n",
        "    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, gradients, learning_rate)\n",
        "    \n",
        "    # Store predictions for visualization\n",
        "    predictions = (p_i > 0.5).astype(int)\n",
        "    predictions_history.append(predictions.flatten())\n",
        "    \n",
        "    # Print progress\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"{epoch}\\t{loss:.4f}\\t\\t{predictions.flatten()}\")\n",
        "\n",
        "print(f\"\\nTraining completed after {num_epochs} epochs!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final evaluation\n",
        "p_i, _ = forward_propagation(X, W1, b1, W2, b2)\n",
        "final_predictions = (p_i > 0.5).astype(int)\n",
        "accuracy = np.mean(final_predictions == y) * 100\n",
        "\n",
        "print(\"Final Results:\")\n",
        "print(f\"Loss: {losses[-1]:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.1f}%\")\n",
        "print(f\"Predictions: {final_predictions.flatten()}\")\n",
        "print(f\"True labels: {y.flatten()}\")\n",
        "\n",
        "# Check if XOR problem is solved\n",
        "if accuracy == 100.0:\n",
        "    print(\"\\n✅ XOR problem successfully solved!\")\n",
        "else:\n",
        "    print(f\"\\n❌ XOR problem not fully solved. Accuracy: {accuracy:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training progress\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot training loss\n",
        "ax1.plot(losses, linewidth=2, color='blue')\n",
        "ax1.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_yscale('log')  # Log scale for better visualization\n",
        "\n",
        "# Plot predictions evolution\n",
        "predictions_array = np.array(predictions_history)\n",
        "for i in range(4):\n",
        "    ax2.plot(predictions_array[:, i], label=f'Input {X[i]}', linewidth=2, marker='o', markersize=3)\n",
        "ax2.set_title('Predictions Evolution', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Prediction (0 or 1)')\n",
        "ax2.set_yticks([0, 1])\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
