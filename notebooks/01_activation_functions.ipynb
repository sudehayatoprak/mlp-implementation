{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activation Functions Demo\n",
        "\n",
        "This notebook demonstrates the implementation and visualization of various activation functions used in neural networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "from activations import relu, sigmoid, tanh, leakyrelu, elu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate input values\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Compute activation functions\n",
        "relu_output = relu(x)\n",
        "sigmoid_output = sigmoid(x)\n",
        "tanh_output = tanh(x)\n",
        "leaky_relu_output = leakyrelu(x, 0.1)\n",
        "elu_output = elu(x, 1.0)\n",
        "\n",
        "print(\"Activation Functions Summary:\")\n",
        "print(f\"ReLU: min={relu_output.min():.3f}, max={relu_output.max():.3f}, mean={relu_output.mean():.3f}\")\n",
        "print(f\"Sigmoid: min={sigmoid_output.min():.3f}, max={sigmoid_output.max():.3f}, mean={sigmoid_output.mean():.3f}\")\n",
        "print(f\"Tanh: min={tanh_output.min():.3f}, max={tanh_output.max():.3f}, mean={tanh_output.mean():.3f}\")\n",
        "print(f\"Leaky ReLU: min={leaky_relu_output.min():.3f}, max={leaky_relu_output.max():.3f}, mean={leaky_relu_output.mean():.3f}\")\n",
        "print(f\"ELU: min={elu_output.min():.3f}, max={elu_output.max():.3f}, mean={elu_output.mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot all activation functions\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "activations = [\n",
        "    (relu_output, 'ReLU'),\n",
        "    (sigmoid_output, 'Sigmoid'),\n",
        "    (tanh_output, 'Tanh'),\n",
        "    (leaky_relu_output, 'Leaky ReLU (α=0.1)'),\n",
        "    (elu_output, 'ELU (α=1.0)')\n",
        "]\n",
        "\n",
        "for i, (output, name) in enumerate(activations):\n",
        "    axes[i].plot(x, output, linewidth=2, color=plt.cm.tab10(i))\n",
        "    axes[i].set_title(name, fontsize=12, fontweight='bold')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "    axes[i].set_xlabel('x')\n",
        "    axes[i].set_ylabel('f(x)')\n",
        "    axes[i].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
        "    axes[i].axvline(x=0, color='k', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Remove the last empty subplot\n",
        "fig.delaxes(axes[5])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
